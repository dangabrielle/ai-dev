# -*- coding: utf-8 -*-
"""aidev_ testing.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1q8kRK5hX7K00AjxULCwxMWCQJaG4WZsa

# Testing different things

# Human PRs are greater than 500 stars, BUT AIDev-pop is only the repos above 100 stars

Create a dataset from the human prs that grab the matching AI agent PRs from the same repossitories and saved it into my Huggging Face:

hf://datasets/kaylamarietorres/AIDev/human_agent_same_repos.parquet
"""

# polars and pyarrow should be installed via: pip install polars pyarrow
import polars as pl

# URLs
human_pr_url = "https://huggingface.co/datasets/hao-li/AIDev/resolve/main/human_pull_request.parquet"
all_pr_url    = "https://huggingface.co/datasets/hao-li/AIDev/resolve/main/all_pull_request.parquet"
repo_url      = "https://huggingface.co/datasets/hao-li/AIDev/resolve/main/all_repository.parquet"

# ----------------------------
# 1. Load data
# ----------------------------
print("Loading datasets...")
human_pr = pl.read_parquet(human_pr_url)
all_pr   = pl.read_parquet(all_pr_url)
repos    = pl.read_parquet(repo_url)

print("Human PR columns:", human_pr.columns)
print("All PR columns:", all_pr.columns)

# ----------------------------
# 1B. Fill missing repo_id for human PRs
# repos.url   = repo_url
# repos.id    = repo_id
# ----------------------------
human_pr = human_pr.join(
    repos.select(["url", "id"]).rename({"url": "repo_url", "id": "repo_id"}),
    on="repo_url",
    how="left"
)

print("After filling, human repo_id null count:",
      human_pr["repo_id"].null_count())

# ---------------------------------------------------------
# Drop human PRs whose repos are missing from repos table
# ---------------------------------------------------------
missing_urls = (
    human_pr
    .filter(pl.col("repo_id").is_null())
    .select("repo_url")
    .unique()
)

missing_list = missing_urls["repo_url"].to_list()

print("\n===== These repos exist in human PRs but NOT in all_repository / all_PR datasets =====")
for url in missing_list:
    print(url)

print(f"\nDropping {len(missing_list)} repos from human_pr...")

# Drop rows where repo_id is null
human_pr = human_pr.filter(pl.col("repo_id").is_not_null())

print("New human_pr shape after dropping:", human_pr.shape)

note = {
    "dropped_repo_urls": missing_list,
    "reason": "These repos appear in human PR dataset but not in all_repository or agent PR datasets."
}

print("\nNOTE:", note)

# ----------------------------
# 2. Identify repos used in Human PR dataset
# ----------------------------
human_repo_ids = human_pr.select("repo_url").unique()
print("\nUnique human repo URLs:", human_repo_ids.height)

if "repo_id" in human_pr.columns:
    human_repo_ids = human_pr.select("repo_id").unique()

# ----------------------------
# 3. Filter agent PRs to those repos
# ----------------------------
print("\nFiltering agent PRs to human PR repos...")

agent_pr = all_pr.filter(pl.col("agent") != "Human")

agent_pr_matched = agent_pr.join(
    human_pr.select("repo_url").unique(),
    on="repo_url",
    how="inner"
)

print("Filtered agent PRs:", agent_pr_matched.shape)

# ----------------------------
# 4. Align schemas before concat
# ----------------------------
print("\n===== Columns BEFORE alignment =====")
print("Human PR columns:", human_pr.columns)
print("Agent PR (matched) columns:", agent_pr_matched.columns)

human_cols = set(human_pr.columns)
agent_cols = set(agent_pr_matched.columns)

missing_in_human = agent_cols - human_cols
missing_in_agent = human_cols - agent_cols

print("Missing in human:", missing_in_human)
print("Missing in agent:", missing_in_agent)

# Add missing cols to human_pr
for col in missing_in_human:
    human_pr = human_pr.with_columns(pl.lit(None).alias(col))

# Add missing cols to agent_pr
for col in missing_in_agent:
    agent_pr_matched = agent_pr_matched.with_columns(pl.lit(None).alias(col))

# Reorder
agent_pr_matched = agent_pr_matched.select(sorted(agent_pr_matched.columns))
human_pr         = human_pr.select(sorted(human_pr.columns))

# ----------------------------
# 5. Concat
# ----------------------------
final_dataset = pl.concat([human_pr, agent_pr_matched], how="vertical_relaxed")

print("\n===== Columns AFTER merge =====")
print("Final dataset columns:", final_dataset.columns)
print("Final dataset size:", final_dataset.shape)

# ----------------------------
# 6. Save to local file
# ----------------------------
output_path = "human_agent_same_repos.parquet"
final_dataset.write_parquet(output_path)

print(f"\nSaved unified dataset to: {output_path}")

"""#### **Summary of Differences Between `repository.parquet` and `all_repository.parquet`**

- `repository.parquet` contains **2,807 repositories** and is a **filtered subset** of the full repository list.  
- `all_repository.parquet` contains **116,211 repositories** and represents the **complete set**.  
- All repository IDs in `repository.parquet` are present in `all_repository.parquet`.  
- `all_repository.parquet` includes ~113k additional repositories not found in the smaller file.  
- The subset appears to include **more popular or higher-visibility repositories** (e.g., higher star counts), but the exact filtering rule is not explicitly documented.

"""

import pandas as pd

repo_url = "https://huggingface.co/datasets/hao-li/AIDev/resolve/main/repository.parquet"
all_repo_url = "https://huggingface.co/datasets/hao-li/AIDev/resolve/main/all_repository.parquet"

# Load both
repo = pd.read_parquet(repo_url)
all_repo = pd.read_parquet(all_repo_url)

print("=== repository.parquet ===")
print("Shape:", repo.shape)
print("Columns:", list(repo.columns))
print(repo.head(), "\n")

print("=== all_repository.parquet ===")
print("Shape:", all_repo.shape)
print("Columns:", list(all_repo.columns))
print(all_repo.head(), "\n")

# Compare overlaps
common_cols = set(repo.columns).intersection(all_repo.columns)
print("Common columns:", common_cols)

print("\nRepository IDs present in repo but not in all_repository:",
      len(set(repo['id']) - set(all_repo['id'])))
print("Repository IDs present in all_repository but not in repo:",
      len(set(all_repo['id']) - set(repo['id'])))

"""#### None of the rows in human_pull_request.parquet appear in all_pull_request.parquet."""

# polars and pyarrow should be installed via: pip install polars pyarrow

import polars as pl

all_pr_url = "https://huggingface.co/datasets/hao-li/AIDev/resolve/main/all_pull_request.parquet"
human_pr_url = "https://huggingface.co/datasets/hao-li/AIDev/resolve/main/human_pull_request.parquet"

print("Downloading Parquet files...")
all_pr = pl.read_parquet(all_pr_url)
human_pr = pl.read_parquet(human_pr_url)

print("\n--- BASIC STATS ---")
print("all_pull_request rows:", all_pr.height)
print("human_pull_request rows:", human_pr.height)

# Check column IDs that can be used for matching
print("\nColumns in all_pull_request:", all_pr.columns)
print("Columns in human_pull_request:", human_pr.columns)

# Assume PRs can be matched by 'id' or 'number' + 'repo_id'
key = None
for candidate in ["id", "pr_id", "number"]:
    if candidate in all_pr.columns and candidate in human_pr.columns:
        key = candidate
        break

if key is None:
    print("\nâŒ No matching key column found to compare PRs directly.")
else:
    print(f"\nUsing matching key column: {key}")

    all_ids = set(all_pr[key].to_list())
    human_ids = set(human_pr[key].to_list())

    overlap = human_ids & all_ids

    print("\n--- CHECK OVERLAP ---")
    print("Number of human PRs also appearing in all_pull_request:", len(overlap))
    print("Example overlapping IDs (if any):", list(overlap)[:10])

"""# multiple agents contribute per repository"""

# polars and pyarrow should be installed via: pip install polars pyarrow
import polars as pl

url = "https://huggingface.co/datasets/kaylamarietorres/AIDev/resolve/main/human_agent_same_repos.parquet"

print("Loading merged dataset...")
df = pl.read_parquet(url)

print(df.shape)
print(df.columns)

# --- Step 1: restrict to agent PRs only (exclude humans)
agents_only = df.filter(pl.col("is_human") == False)

# --- Step 2: For each repo, count number of distinct agents
agents_per_repo = (
    agents_only
    .group_by("repo_url")
    .agg([
        pl.col("agent").n_unique().alias("num_agents"),
        pl.col("agent").unique().alias("agents_used"),
        pl.count().alias("num_agent_prs")
    ])
    .sort("num_agents", descending=True)
)

print("Repositories using more than one agent:")
print(agents_per_repo.filter(pl.col("num_agents") > 1).head(20))

