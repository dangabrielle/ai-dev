# -*- coding: utf-8 -*-
"""HumanDev.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/github/awjans/CopilotForPRsAdoption/blob/main/scripts/HumanDev.ipynb

# Data Collection/Cleaning Overview
1. **PR identification**
   * Queried GitHub via GraphQL for PRs whose description contained the phrase **â€œGenerated by Copilotâ€** or any of the marker tags:

     * `copilot:summary`
     * `copilot:walkthrough`
     * `copilot:poem`
     * `copilot:all`

2. **Scope**
   * Collected **18,256 PRs** from **146 early-adopter repositories** during **March 2023 â€“ August 2023**.

3. **Control set**
   * For the same repositories, gathered **54,188 PRs** that did **not** contain any Copilot marker.
   * These served as the **untreated (control) group** for the **RQ2 comparison**.

4. **Bot filtering**
   * Removed PRs and comments authored by bots using the **high-precision method** of **Golzadeh et al. (2022)**, which included:
     * (i) Usernames ending with â€œbotâ€
     * (ii) A curated list of **527 known bot accounts**

5. **Revision extraction (RQ3)**
   * From the **18,256 Copilot-generated PRs**, retrieved the full **edit history** of PR descriptions.
   * Identified **1,437 revisions** where developers **edited the AI-suggested content**.
"""

# Toggle to TRUE to limit dataset, else process entire dataset
TEST_MODE = True
# Toggle to TRUE to reload from disk, else do the preprocessing
RELOAD = False  # Changed to False to check original data

import os

import os

# Make script runnable locally: use the workspace `AIDev_shared` directory
BASE_DIR = os.path.abspath(os.path.dirname(__file__))
DATA_FOLDER = os.path.join(BASE_DIR, 'AIDev_shared')
os.makedirs(DATA_FOLDER, exist_ok=True)

MODIFIER = '_TEST' if TEST_MODE else ''

METRICS_PKL = os.path.join(DATA_FOLDER, f'metrics_hu{MODIFIER}.pkl')
METRICS_CSV = os.path.join(DATA_FOLDER, f'metrics_hu{MODIFIER}.csv')
REPOS_PKL = os.path.join(DATA_FOLDER, f'repos_hu{MODIFIER}.pkl')

OUTPUT_CSV = os.path.join(DATA_FOLDER, f'control_metrics{MODIFIER}.csv')

import asyncio
import matplotlib.pyplot as plt
import nest_asyncio
import numpy as np
import pandas as pd
import requests
import seaborn as sns
import datetime
from dateutil import parser
from urllib.parse import urlparse
import time
import random
from itertools import cycle

# GitHub API Configuration
GITHUB_TOKEN = os.getenv('GITHUB_TOKEN', None)
RATE_LIMIT_DELAY = 1  # seconds between API requests
USE_GITHUB_API = True  # Set to False to skip API calls and use 0's for missing data

def parse_pr_url(html_url):
    """
    Extract owner, repo, and PR number from GitHub PR URL
    Example: https://github.com/owner/repo/pull/123
    """
    parts = html_url.rstrip('/').split('/')
    if len(parts) >= 4 and 'github.com' in html_url:
        owner = parts[-4]
        repo = parts[-3]
        pr_number = parts[-1]
        return owner, repo, pr_number
    return None, None, None

def fetch_pr_details_from_api(owner, repo, pr_number, pr_author=None):
    """
    Fetch PR details from GitHub API including comment breakdown and reviewers
    Returns: dict with additions, deletions, changed_files, commits, comments breakdown, reviewers count
    """
    headers = {'Accept': 'application/vnd.github.v3+json'}
    if GITHUB_TOKEN:
        headers['Authorization'] = f'token {GITHUB_TOKEN}'

    url = f'https://api.github.com/repos/{owner}/{repo}/pulls/{pr_number}'

    try:
        # Get basic PR data
        response = requests.get(url, headers=headers, timeout=10)

        if response.status_code == 404:
            return {'status': 'not_found', 'error': 'PR not found'}
        elif response.status_code == 403:
            return {'status': 'rate_limited', 'error': 'Rate limit exceeded'}
        elif response.status_code != 200:
            return {'status': 'error', 'error': f'HTTP {response.status_code}'}

        data = response.json()
        pr_author = pr_author or data.get('user', {}).get('login', '')

        # Get issue comments (general PR discussion)
        comments_url = f'https://api.github.com/repos/{owner}/{repo}/issues/{pr_number}/comments'
        comments_response = requests.get(comments_url, headers=headers, timeout=10)

        author_comments = 0
        reviewer_comments = 0
        total_comments = 0

        if comments_response.status_code == 200:
            comments = comments_response.json()
            total_comments = len(comments)
            for comment in comments:
                comment_author = comment.get('user', {}).get('login', '')
                if comment_author == pr_author:
                    author_comments += 1
                else:
                    reviewer_comments += 1

        # Get PR reviews to count unique reviewers (excluding PR author)
        reviews_url = f'https://api.github.com/repos/{owner}/{repo}/pulls/{pr_number}/reviews'
        reviews_response = requests.get(reviews_url, headers=headers, timeout=10)

        reviewers_total_count = 0
        if reviews_response.status_code == 200:
            reviews = reviews_response.json()
            # Get unique reviewers (excluding the PR author)
            unique_reviewers = set()
            for review in reviews:
                reviewer = review.get('user', {}).get('login', '')
                if reviewer and reviewer != pr_author:
                    unique_reviewers.add(reviewer)
            reviewers_total_count = len(unique_reviewers)

        return {
            'additions': data.get('additions', 0),
            'deletions': data.get('deletions', 0),
            'changed_files': data.get('changed_files', 0),
            'commits': data.get('commits', 0),
            'comments_total': total_comments,
            'author_comments': author_comments,
            'reviewer_comments': reviewer_comments,
            'reviewers_total_count': reviewers_total_count,
            'status': 'success'
        }
    except Exception as e:
        return {'status': 'error', 'error': str(e)}

def check_rate_limit():
    """Check remaining GitHub API rate limit"""
    headers = {'Accept': 'application/vnd.github.v3+json'}
    if GITHUB_TOKEN:
        headers['Authorization'] = f'token {GITHUB_TOKEN}'

    url = 'https://api.github.com/rate_limit'
    try:
        response = requests.get(url, headers=headers, timeout=10)
        if response.status_code == 200:
            data = response.json()
            core = data['resources']['core']
            remaining = core['remaining']
            limit = core['limit']
            reset_time = datetime.datetime.fromtimestamp(core['reset'])

            print(f"\nðŸ“Š API Rate Limit: {remaining}/{limit} remaining")
            print(f"   Resets at: {reset_time}")
            return remaining, reset_time
    except:
        pass
    return None, None

"""# **First**, Define the URLs of the AIDev Parquet Files that we are intersted in."""

pull_request_file_url = 'https://huggingface.co/datasets/hao-li/AIDev/resolve/main/human_pull_request.parquet'
pr_comments_file_url = 'https://huggingface.co/datasets/hao-li/AIDev/resolve/main/pr_comments.parquet'
pr_commits_file_url = 'https://huggingface.co/datasets/hao-li/AIDev/resolve/main/pr_commits.parquet'
pr_commit_details_file_url = 'https://huggingface.co/datasets/hao-li/AIDev/resolve/main/pr_commit_details.parquet'
pr_reviews_file_url = 'https://huggingface.co/datasets/hao-li/AIDev/resolve/main/pr_reviews.parquet'
pr_review_comments_file_url = 'https://huggingface.co/datasets/hao-li/AIDev/resolve/main/pr_review_comments.parquet'
pr_task_type_file_url = 'https://huggingface.co/datasets/hao-li/AIDev/resolve/main/pr_task_type.parquet'
repository_file_url = 'https://huggingface.co/datasets/hao-li/AIDev/resolve/main/all_repository.parquet'
user_file_url = 'https://huggingface.co/datasets/hao-li/AIDev/resolve/main/user.parquet'

"""# **Second**, We need to load the data from the URLs (15s)

### Load Parquet Files
"""

"""
Load the Parquet file into a Pandas DataFrame from the file URL.
"""
def load_data(url: str):
  import pandas as pd # Import pandas inside the function
  try:
    # For Parquet files:
    df = pd.read_parquet(url)

    return df
  except Exception as e:
      print(f"Error loading data: {e}")
      print("Please ensure the URL is correct and the file is publicly accessible.")
      return None # Return None in case of an error

if not RELOAD:
  # Load the data from the URLs
  pull_request = load_data(pull_request_file_url)
  repository = load_data(repository_file_url)
  pr_comments = load_data(pr_comments_file_url)
  pr_commits = load_data(pr_commits_file_url)
  pr_commit_details = load_data(pr_commit_details_file_url)
  pr_reviews = load_data(pr_reviews_file_url)
  pr_review_comments = load_data(pr_review_comments_file_url)
  pr_task_type = load_data(pr_task_type_file_url)
  user = load_data(user_file_url)
else:
  # When reloading, we still need these dataframes for processing
  # Only load them if we're reloading from pickles
  pr_comments = load_data(pr_comments_file_url)
  pr_commits = load_data(pr_commits_file_url)
  pr_commit_details = load_data(pr_commit_details_file_url)
  pr_reviews = load_data(pr_reviews_file_url)
  pr_review_comments = load_data(pr_review_comments_file_url)
  pr_task_type = load_data(pr_task_type_file_url)
  user = load_data(user_file_url)

"""## Create a Copy of Pull_Requests & Data Cleaning

1. **Copies DataFrames:** It creates copies of the pull_request and repository DataFrames and assigns them to metrics and repos respectively. This is a good practice to avoid modifying the original loaded data.
2. **Renames Columns:** It renames the 'id' column to 'pr_id' in the metrics DataFrame and to 'repo_id' in the repos DataFrame. This is done to prepare for merging these DataFrames later.
3. **Filters Open Pull Requests:** It removes pull requests that are still open by filtering out rows where the 'closed_at' column has a missing value (NaN).
4. **Converts Timestamps:** It converts the 'created_at' and 'closed_at' columns in the metrics DataFrame to datetime objects. This allows for easier time-based calculations.
5. **Filters Repositories:** It removes repositories from the repos DataFrame that do not have any closed pull requests in the metrics DataFrame.
6. **Gets Repository Creation Dates:** For the remaining repositories, it calls the get_repo_created_at function (defined in a previous cell) to fetch the creation date of each repository from the GitHub API and stores it in a new column 'repo_created_at'.
7. **Filters Repositories with Creation Dates:** It removes repositories where the 'repo_created_at' could not be retrieved.
"""

import os, time, random, requests, pandas as pd
from datetime import datetime
from itertools import cycle # Import cycle

if not RELOAD:
  # Copy raw data
  metrics = pull_request.copy()
  repos = repository.copy()

  # === BASIC CLEANUP ===
  metrics = metrics.rename(columns={'id': 'pr_id'})
  repos = repos.rename(columns={'id': 'repo_id', 'url': 'repo_url'})

  print(f"Total PRs before filtering: {len(metrics):,}")
  metrics = metrics[metrics['closed_at'].notna()]
  metrics = pd.merge(metrics, repos[['repo_url','repo_id']], on='repo_url', how='left')
  metrics = metrics[metrics['repo_id'].notna()]
  print(f"Closed PRs retained: {len(metrics):,}")

  metrics['created_at'] = pd.to_datetime(metrics['created_at'], errors='coerce')
  metrics['closed_at'] = pd.to_datetime(metrics['closed_at'], errors='coerce')

  print(f"Total repos before filtering: {len(repos):,}")
  repos = repos[repos['repo_id'].isin(metrics['repo_id'])]
  print(f"Repos with â‰¥1 PR: {len(repos):,}")

  # === ADD: SMALL-SUBSET TEST MODE ===
  if TEST_MODE:
      # Select 50 repos (from index 0 to 50)
      start_index = 0
      end_index = 50
      if end_index > len(repos):
          end_index = len(repos)
          print(f"[TEST MODE] Adjusting end index to {end_index} as it exceeds the number of repos.")

      test_repo_ids = repos['repo_id'].iloc[start_index:end_index]

      metrics = metrics[metrics['repo_id'].isin(test_repo_ids)]
      repos = repos[repos['repo_id'].isin(test_repo_ids)]
      print(f"[TEST MODE] Restricting to {len(repos)} repos and {len(metrics)} PRs (indices {start_index} to {end_index-1})")

"""Save initial processed Dataset (no need to re-run this every time)"""

if not RELOAD:
  metrics.to_pickle(METRICS_PKL)
  repos.to_pickle(REPOS_PKL)

"""Reload dataset (this can be ran every time)"""

if RELOAD:
  try:
    metrics = pd.read_pickle(METRICS_PKL)
    repos = pd.read_pickle(REPOS_PKL)
    print('âœ… Loaded shared cached dataset (pickles)')
  except Exception as e:
    print(f"Pickle load failed: {e}")
    # Fallback: try reading CSV for metrics, and create empty repos if absent
    if os.path.exists(METRICS_CSV):
      metrics = pd.read_csv(METRICS_CSV)
      print('âœ… Loaded metrics from CSV')
    else:
      raise FileNotFoundError(f'No cached pickle or CSV found for metrics in {DATA_FOLDER}')

    if os.path.exists(REPOS_PKL):
      try:
        repos = pd.read_pickle(REPOS_PKL)
        print('âœ… Loaded repos from pickle')
      except Exception:
        repos = pd.DataFrame()
        print('âš ï¸  Failed to read repos pickle; using empty repos DataFrame')
    else:
      repos = pd.DataFrame()
      print('âš ï¸  No repos pickle found; using empty repos DataFrame')

  # Convert datetime columns after loading (important for CSV loads)
  if 'created_at' in metrics.columns:
    metrics['created_at'] = pd.to_datetime(metrics['created_at'], errors='coerce')
  if 'closed_at' in metrics.columns:
    metrics['closed_at'] = pd.to_datetime(metrics['closed_at'], errors='coerce')
  if 'merged_at' in metrics.columns:
    metrics['merged_at'] = pd.to_datetime(metrics['merged_at'], errors='coerce')

"""# **Third**, Gather the covariant variables

## PR Variables

1. **additions:** The # of added LOC by a PR
2. **deletions:** The # of deleted LOC by a PR
3. **prSize:** The total number of added and deleted LOC by a PR (additions + deletions)
4. **purpose:** The purpose of a PR, i.e., bug, document, and feature. Simple keyword search in the title/body ('fix', 'bug', 'doc', â€¦).
5. **changedFiles:** The # of files changed by a PR
6. **commitsTotalCount:** The # of commits involved in a PR
7. **bodyLength**: Length of the PR body (in characters).
8. **prExperience:** The # of prior PRs that were submitted by the PR author (authorâ€™s prior PR count). Query the authorâ€™s PR history in the same repo and count PRs created before the current one.
9. **commentsTotalCount:** The # of comments left on a PR
10. **authorComments:** The # of comments left by the PR author
11. **reviewersComments:** The # of comments left by the reviewers who participate in the disucssion
12. **reviewersTotalCount:** The # of developers who participate in the discussion (excluding author).
13. **repoAge:** Time interval between the repository creation time and PR creation time in days.
14. **isMember:** Whether or not the author is a member or outside collaborator (True/False).
15. **state**: State of the pull request (MERGED or CLOSED).
16. **reviewTime**: Time taken to review the PR (in hours, floating point, no rounding).

## Project variables

17. **repoLanguage:** Programming language of the repository (e.g., Python, PHP, TypeScript, Vue). *[I'm assuming its the top language as there is only one]*
18. **forkCount:** The # of forks that a repository has
19. **stargazerCount:** The # of stargazers that a repository has.

## Treatment variables

20. **With Copilot for PRs:** Whether or not a PR is generated by Copilot for PRs (binary)

## Outcome variables

21. **Review time (reviewTime):** Time interval between the PR creation time and closed time in hours
22. **Is merged (state):** Whether or not a PR is merged (binary)

# PR Variables

1. **additions:** The # of added LOC by a PR
2. **deletions:** The # of deleted LOC by a PR
3. **prSize:** The total number of added and deleted LOC by a PR (additions + deletions)

**Xiao 2024:**

In the notebook (e.g., CollectCopilot4prs.ipynb), the `additions` and `deletions` values are extracted directly from the GitHub API response for each PR: `pr['node']['additions']` and `pr['node']['deletions']`. The GraphQL query for PRs includes the fields, so the value is as reported by GitHub. `prSize = additions + deletions`


**Our Approach:**

In the `pr_commit_details` DataFrame, we use the `additions` and `deletions` fields. We sum them for `prSize`. Alternatively, the dataset also has`changes` which represents prSize but we chose to perform the sum ourselves.
"""

# Make sure we don't crash because the columns already exist (reentrant code)
metrics = metrics.drop(columns=['additions', 'deletions', 'prSize', 'changedFiles', 'commitsTotalCount',
                                'commentsTotalCount', 'authorComments', 'reviewersComments', 'reviewersTotalCount'],
                      errors='ignore')

print(f"Number of PRs before adding GitHub API metrics: {len(metrics):,}")

# Fetch metrics from GitHub API for each PR
if USE_GITHUB_API:
    print("Fetching PR metrics from GitHub API...")
    if GITHUB_TOKEN:
        print(" Using GitHub token for authentication (5000 requests/hour)")
    else:
        print("  No GitHub token found. Rate limit: 60 requests/hour")
        print("   Set GITHUB_TOKEN environment variable for better rate limits")

    check_rate_limit()

    api_results = []
    total_prs = len(metrics)

    for idx, row in metrics.iterrows():
        pr_id = row['pr_id']
        html_url = row['html_url']
        pr_author = row.get('user', None)

        # Parse URL
        owner, repo, pr_number = parse_pr_url(html_url)

        if not all([owner, repo, pr_number]):
            print(f" [{idx+1}/{total_prs}] Invalid URL: {html_url}")
            api_results.append({
                'pr_id': pr_id,
                'additions': 0,
                'deletions': 0,
                'changed_files': 0,
                'commits': 0,
                'comments_total': 0,
                'author_comments': 0,
                'reviewer_comments': 0,
                'reviewers_total_count': 0,
                'fetch_status': 'invalid_url'
            })
            continue

        # Fetch from API
        if (idx + 1) % 10 == 1:  # Progress update every 10 PRs
            print(f"Fetching {owner}/{repo}#{pr_number}...", end=' ')

        pr_data = fetch_pr_details_from_api(owner, repo, pr_number, pr_author)

        if pr_data['status'] == 'success':
            if (idx + 1) % 10 == 1:
                print(f" +{pr_data['additions']} -{pr_data['deletions']} files:{pr_data['changed_files']}")
            api_results.append({
                'pr_id': pr_id,
                'additions': pr_data['additions'],
                'deletions': pr_data['deletions'],
                'changed_files': pr_data['changed_files'],
                'commits': pr_data['commits'],
                'comments_total': pr_data['comments_total'],
                'author_comments': pr_data['author_comments'],
                'reviewer_comments': pr_data['reviewer_comments'],
                'reviewers_total_count': pr_data['reviewers_total_count'],
                'fetch_status': 'success'
            })
        elif pr_data['status'] == 'rate_limited':
            print(f"\nRate limited! Stopping API calls. Remaining PRs will have 0 values.")
            print("   Please wait for rate limit to reset or use a GitHub token.")
            # Fill remaining PRs with 0s
            for remaining_idx in range(idx, len(metrics)):
                remaining_row = metrics.iloc[remaining_idx]
                api_results.append({
                    'pr_id': remaining_row['pr_id'],
                    'additions': 0,
                    'deletions': 0,
                    'changed_files': 0,
                    'commits': 0,
                    'comments_total': 0,
                    'author_comments': 0,
                    'reviewer_comments': 0,
                    'reviewers_total_count': 0,
                    'fetch_status': 'rate_limited'
                })
            break
        else:
            if (idx + 1) % 10 == 1:
                print(f" {pr_data.get('error', 'Unknown error')}")
            api_results.append({
                'pr_id': pr_id,
                'additions': 0,
                'deletions': 0,
                'changed_files': 0,
                'commits': 0,
                'comments_total': 0,
                'author_comments': 0,
                'reviewer_comments': 0,
                'reviewers_total_count': 0,
                'fetch_status': pr_data['status']
            })

        # Rate limiting delay
        time.sleep(RATE_LIMIT_DELAY)

        # Check rate limit every 50 requests
        if (idx + 1) % 50 == 0:
            check_rate_limit()

    # Convert to DataFrame
    api_results_df = pd.DataFrame(api_results)

    # Merge with metrics
    metrics = pd.merge(metrics, api_results_df[['pr_id', 'additions', 'deletions', 'changed_files', 'commits',
                                                'comments_total', 'author_comments', 'reviewer_comments',
                                                'reviewers_total_count']],
                      on='pr_id', how='left')

    # Rename columns to match expected names
    metrics = metrics.rename(columns={
        'changed_files': 'changedFiles',
        'commits': 'commitsTotalCount',
        'comments_total': 'commentsTotalCount',
        'author_comments': 'authorComments',
        'reviewer_comments': 'reviewersComments',
        'reviewers_total_count': 'reviewersTotalCount'
    })

    # Calculate prSize (additions + deletions)
    metrics['prSize'] = metrics['additions'] + metrics['deletions']

    print(f"\nAPI fetch complete!")
    print(f"   Successful fetches: {(api_results_df['fetch_status'] == 'success').sum()}/{len(api_results_df)}")
    print(f"   Average additions: {metrics['additions'].mean():.1f}")
    print(f"   Average deletions: {metrics['deletions'].mean():.1f}")
    print(f"   Average changedFiles: {metrics['changedFiles'].mean():.1f}")
else:
    print("  USE_GITHUB_API is False. Setting all metrics to 0")
    metrics['additions'] = 0
    metrics['deletions'] = 0
    metrics['prSize'] = 0
    metrics['changedFiles'] = 0
    metrics['commitsTotalCount'] = 0
    metrics['commentsTotalCount'] = 0
    metrics['authorComments'] = 0
    metrics['reviewersComments'] = 0
    metrics['reviewersTotalCount'] = 0

# Fill N/A values with defaults
metrics['additions'] = metrics['additions'].fillna(0).astype(int)
metrics['deletions'] = metrics['deletions'].fillna(0).astype(int)
metrics['prSize'] = metrics['prSize'].fillna(0).astype(int)
metrics['changedFiles'] = metrics['changedFiles'].fillna(0).astype(int)
metrics['commitsTotalCount'] = metrics['commitsTotalCount'].fillna(0).astype(int)
metrics['commentsTotalCount'] = metrics['commentsTotalCount'].fillna(0).astype(int)
metrics['authorComments'] = metrics['authorComments'].fillna(0).astype(int)
metrics['reviewersComments'] = metrics['reviewersComments'].fillna(0).astype(int)
metrics['reviewersTotalCount'] = metrics['reviewersTotalCount'].fillna(0).astype(int)

print(f"Number of PRs after adding GitHub API metrics: {len(metrics):,}")

"""4. **purpose:** The purpose of a PR, i.e., bug, document, and feature. Simple keyword search in the title/body ('fix', 'bug', 'doc', â€¦).

**Xiao 2024:**

In `CollectCopilot4prs.ipynb`, the code uses `np.select` with conditions based on the PR's title and body content to assign "Bug", "Document", or "Feature" as the purpose. This is a simple rule-based classification:

- If the title/body contains keywords for bugs (e.g., "fix", "bug"), it's labeled "Bug".
- If it contains documentation keywords (e.g., "doc"), it's labeled "Document".
- Otherwise, it's labeled "Feature".


**Our approach:**

The `title` and `body` columns are part of the initial dataset that was loaded into the pull_request (`all_pull_request.parquet`) DataFrame.

"""

# Make sure we don't crash because the columns already exist (reentrant code)
metrics = metrics.drop(columns=['purpose'], errors='ignore')

print(f"Number of PRs before calculating purpose: {len(metrics):,}")

# Combine title and body for keyword search, handling potential None values
metrics['title_body'] = metrics['title'].fillna('') + ' ' + metrics['body'].fillna('')

# Define conditions and choices for np.select
conditions = [
    metrics['title_body'].str.contains('fix|bug', case=False, na=False),
    metrics['title_body'].str.contains('doc', case=False, na=False)
]
choices = ['fix', 'doc']

# Apply np.select to determine purpose
metrics['purpose'] = np.select(conditions, choices, default='feat')

# Drop the temporary combined column
metrics = metrics.drop(columns=['title_body'])

print(f"Number of PRs after calculating purpose: {len(metrics):,}")

"""5. **changedFiles:** The # of files changed by a PR

**Xiao 2024:**

The `changedFiles` field is extracted directly from the GitHub API for each pull request. In the code (e.g., `in CollectCopilot4prs.ipynb`), it is accessed as: `pr['node']['changedFiles']`.


**Our Approach:**


This variable is calculated from the `pr_commit_details` DataFrame, steps include:

- Identify the PR ID: The code uses `groupby(['pr_id', 'filename'])` which implicitly identifies each PR by its `pr_id`.
- Locate the file-level change records: It operates on the `pr_commit_details` DataFrame, which contains the file-level change records.
- Collect all rows belonging to the same PR: The `groupby(['pr_id', 'filename'])` operation groups all rows for a specific PR together.
- Count the number of unique filenames for each `pr_id` across all its commits.
"""

# NOTE: changedFiles is now fetched from GitHub API above
# (Old pr_commit_details-based code removed)

"""6. **commitsTotalCount:** The # of commits involved in a PR


**Xiao, 2024:**

Fetched from GitHubâ€™s GraphQL API by querying the PRâ€™sâ€¯`commits { totalCount }` field.

**Our Approach:**

The `pr_commit_details` table contains a `sha` column (the commit hash) and a `pr_id` column that links each commit to its pull request. Count every distinct `sha` in the entire table. Group by `pr_id` and count distinct `sha` values for each group.

"""

# NOTE: commitsTotalCount is now fetched from GitHub API above
# (Old pr_commit_details-based code removed)

"""7. **bodyLength:** The length of a PR description

**Xiao, 2024:**

Query each PR with `pullRequest { body }`. Take the returned text and compute its character count (e.g., len(body)). Record that count as the value of description length.



**Our Approach:**

In the `pull_request` DataFrame, calculate the character length of the `body` column.

"""

# Make sure we don't crash because the columns already exist (reentrant code)
metrics = metrics.drop(columns=['bodyLength'], errors='ignore')

print(f"Number of PRs before adding bodyLength: {len(metrics):,}")

# Get the Length of the Body of the Pull Request
metrics['bodyLength'] = metrics['body'].str.len()

print(f"Number of PRs after adding bodyLength: {len(metrics):,}")

"""8. **prExperience:** The # of prior PRs that were submitted by the PR author (authorâ€™s prior PR count). Query the authorâ€™s PR history in the same repo and count PRs created before the current one.


**Xiao 2024:**

For every pull request the study queries GitHubâ€™s GraphQL API and extracts the author.login (or author.id) and the repository identifier (repository.id). Using the same API they request all pull requests belonging to the same repository.id whose author.login matches the author of the target PR. Each of these PRs includes its createdAt timestamp. The list is filtered to keep only those PRs whose createdAt value is earlier than the createdAt timestamp of the target PR. The number of remaining PRs is taken as an integer count.




**Our Approach:**

- Extract the author's login from the `user` column.
- Sorts the metrics DataFrame by `repo_id`, `author_login`, and the PR creation time (`created_at`).
- Groups the sorted DataFrame by both `repo_id` and `author_login`.
- Within each group (for each unique author in each unique repository), it uses the `.cumcount()` method. `cumcount()` assigns a sequential number starting from 0 to each row within the group based on the current order (which is sorted by `created_at`).



"""

# Make sure we don't crash because the columns already exist (reentrant code)
metrics = metrics.drop(columns=['prExperience'], errors='ignore')

print(f"Number of PRs before adding prExperience: {len(metrics):,}")

# Extract the author's login from the 'user' column and store it in a new column 'author_login'
metrics['author_login'] = metrics['user'].astype(str).str.strip()

# Drop rows where 'repo_id' or 'created_at' are missing, as these are needed for sorting and calculation
metrics = metrics.dropna(subset=['repo_id', 'created_at'])

# Sort the DataFrame by 'repo_id', 'author_login', and 'created_at' in ascending order--This is crucial for correctly calculating the cumulative count of PRs for each author within each repository.
metrics = metrics.sort_values(['repo_id', 'author_login', 'created_at'])

# Calculate the cumulative count of PRs for each author within each repository.
# The `groupby(['repo_id', 'author_login'])` groups the DataFrame by repository and author.
# The `cumcount()` method then calculates the number of previous PRs for each row within those groups.
metrics['prExperience'] = (
    metrics.groupby(['repo_id', 'author_login'])
           .cumcount()
           .astype('Int64')
)

print(f"Number of PRs after adding bodyLength: {len(metrics):,}")

"""9. **commentsTotalCount:** The # of comments left on a PR



**Xiao 2024:**

`commentsTotalCount` is obtained directly from the GitHub GraphQL API. For each pull request it queries the PRâ€™s `comments` connection and records the `totalCount` field, which is the number of comment objects attached to that PR (including review comments, issueâ€‘style comments, and any other discussion entries).



**Our Approach:**

Group the `pr_comments` DataFrame by `pr_id` (Pull Request ID). Each row in `pr_comments` represents a single comment.
Count the number of rows within each group using `.size()`.




"""

# NOTE: commentsTotalCount is now fetched from GitHub API above
# (Old pr_comments-based code removed)

"""10. **authorComments:** The # of comments left by the PR author

**Xiao 2024:**

Query each PRâ€™s comment data through the GitHub GraphQL API and then filter the comment list to keep only those whose `author.login` matches the PR authorâ€™s login.

**Our Approach:**

Merges `pr_comments` with author information and counting comments where the comment author matches the PR author.
"""

# NOTE: authorComments is now fetched from GitHub API above
# (Old pr_comments-based code removed)

"""11. **reviewersComments:** The # of comments left by the reviewers who participate in the disucssion



**Xiao 2024:**

Querying the `comments` edge of each PR via GraphQL (or the REST endpoint `GET /repos/{owner}/{repo}/issues/{pull_number}/comments`). Filtering out comments whose user.login equals the PR authorâ€™s login. Counting the remaining comments â€“ that number is the reviewerComments value.


**Our Approach:**

Merges `pr_comments` with author information and counting comments where the comment author does not match the PR author.


"""

# NOTE: reviewersComments is now fetched from GitHub API above
# (Old pr_comments-based code removed)

"""12. **reviewersTotalCount:** The # of developers who participate in the discussion (excluding author).

**Xiao 2024:**

Calling the GraphQL endpoint for each PR,
Requesting the `reviewers` (or `reviewRequests`) connection,
Reading the `totalCount` field,
Verifying that the authorâ€™s login is excluded (or simply using the `totalCount` as GitHub already excludes the author in the `reviewers` connection).

**Our Approach:**

Calculates `reviewersTotalCount` by summing the counts of unique reviewers from reviews (`reviewers`) for each pull request.


"""

# NOTE: reviewersTotalCount is now fetched from GitHub API above
# (Old pr_reviews-based code removed)

# Display unique values and their counts for 'reviewersTotalCount'
print("\nUnique values and counts for 'reviewersTotalCount':")
print(metrics['reviewersTotalCount'].value_counts(dropna=False).sort_index())

"""13. **repoAge:** Time interval between the repository creation time and PR creation time in days.


**Xiao 2024:**

- Retrieve the creation timestamp of the repository (the time the repo was first created on GitHub).
- Retrieve the creation timestamp of the pull request under study.
- Compute the time interval between these two timestamps in days



**Our Approach:**


Separate Process

"""

def repo_age(repo_url: str):
  if not repo_url:
    return 0
  return 0

# Ensure we don't crash if columns already exist (reentrant code)
metrics = metrics.drop(columns=['repoAge'], errors='ignore')

metrics['repoAge'] = metrics['repo_url'].apply(lambda x: repo_age(x))

"""14. **isMember:** Whether or not the author is a member or outside collaborator (True/False).


**Xiao 2024:**

- For each PR the study calls GitHubâ€™s GraphQL API and retrieves the authorâ€™s association with the repository (the `authorAssociation` field).
- If the returned association is `MEMBER` or `OWNER`, the flag is set toâ€¯1; otherwise (e.g., `CONTRIBUTOR`, `NONE`, or an external collaborator) it is set toâ€¯0.


**Our Approach:**

Followed the same approach as Xiao 2024.
"""

def is_member(pr_url: str):
  if not pr_url:
    return False
  return False

# Ensure we don't crash if columns already exist (reentrant code)
metrics = metrics.drop(columns=['isMember'], errors='ignore')

metrics['isMember'] = metrics['html_url'].apply(lambda x: is_member(x))

"""15. **state**: State of the pull request (MERGED or CLOSED).


**Xiao 2024:**

In GitHub GraphQL (or REST) API, the response includes a field called state (or, equivalently, a Boolean merged flag). The value of that field can be one of three mutuallyâ€‘exclusive statuses:
- MERGED (or merged = true)	The PR was successfully merged into the target branch.
- CLOSED (or merged = falseâ€¯&â€¯state = CLOSED)	The PR was closed without being merged.
- OPEN (or state = OPEN)	The PR was still open at the time the data were collected.


**Our Approach:**

If the `merged_at` column for a pull request has a value (i.e., it's not `null`), it means the pull request was merged, and the state is set to `MERGED`.
If the `merged_at` column is `null`, it means the pull request was closed without being merged, and the state is set to `CLOSED`.



"""

# Make sure we don't crash because the columns already exist (reentrant code)
metrics = metrics.drop(columns=['state'], errors='ignore')

print(f"Number of PRs before adding state: {len(metrics):,}")

# Set the State to MERGED or CLOSED
metrics['state'] = metrics['merged_at'].apply(lambda x: 'MERGED' if x is not None else 'CLOSED')

print(f"Number of PRs after adding state: {len(metrics):,}")

"""16. **reviewTime**: Time taken to review the PR (in hours, floating point, no rounding).

**Xiao 2024:**

`reviewtime` (in hours) = (PR Closed Timestamp - PR Creation Timestamp).


**Our Approach:**

The difference between the `closed_at` and `created_at` timestamps and converting that duration into hours.



"""

# Make sure we don't crash because the columns already exist (reentrant code)
metrics = metrics.drop(columns=['reviewTime'], errors='ignore')

print(f"Number of PRs before adding reviewTime: {len(metrics):,}")

# Calculate the Review Time
metrics['reviewTime'] = (metrics['closed_at'] - metrics['created_at']).dt.total_seconds() / 3600

print(f"Number of PRs after adding reviewTime: {len(metrics):,}")

"""# Project variables

17. **repoLanguage:** Programming language of the repository (e.g., Python, PHP, TypeScript, Vue).

**Xiao 2024:**

The number of stargazers that a repository has

**Our Approach:**

Same approach


18. **forkCount:** The # of forks that a repository has

**Xiao 2024:**


**Our Approach:**


19. **stargazerCount:** The # of stargazers that a repository has.

**Xiao 2024:**


**Our Approach:**
"""

# Make sure we don't crash because the columns already exist (reentrant code)
metrics = metrics.drop(columns=['repoLanguage', 'forkCount', 'stargazerCount'], errors='ignore')

# Only process repo data if we have a non-empty repos DataFrame
if not repos.empty and len(repos) > 0:
    repos_temp = repos.copy()

    # Check if repos has the necessary columns and rename if needed
    if 'id' in repos_temp.columns and 'repo_id' not in repos_temp.columns:
        repos_temp = repos_temp.rename(columns={'id': 'repo_id'})

    # Drop and rename columns
    repos_temp = (repos_temp
                       .drop(columns=['license', 'repo_url', 'html_url', 'full_name'], errors='ignore')
                       .rename(columns={'language': 'repoLanguage', 'forks': 'forkCount', 'stars': 'stargazerCount'}))

    # Group by ID and get the First Record only if repo_id exists
    if 'repo_id' in repos_temp.columns:
        repos_temp = repos_temp.groupby(['repo_id']).first().reset_index()
        # Merge the Dataframes using a left join
        metrics = pd.merge(metrics, repos_temp, left_on='repo_id', right_on='repo_id', how='left')
    else:
        print("Warning: repo_id column not found in repos. Skipping merge.")

    # Garbage Collect the temporary Dataframe
    repos_temp = None
else:
    print("Warning: repos DataFrame is empty. Skipping repository metrics merge.")

# Fill N/A values with defaults
if 'repoLanguage' not in metrics.columns:
    metrics['repoLanguage'] = 'other'
else:
    metrics['repoLanguage'] = metrics['repoLanguage'].fillna('other')

if 'forkCount' not in metrics.columns:
    metrics['forkCount'] = 0
else:
    metrics['forkCount'] = metrics['forkCount'].fillna(0).astype(int)

if 'stargazerCount' not in metrics.columns:
    metrics['stargazerCount'] = 0
else:
    metrics['stargazerCount'] = metrics['stargazerCount'].fillna(0).astype(int)

"""# Treatment variables

21. **With Copilot for PRs:** Whether or not a PR is generated by Copilot for PRs (binary)

These should not be by an AI Agent
"""

# Make sure we don't crash because the columns already exist (rentrant code)
metrics = metrics.drop(columns=['isGeneratedByCopliot'], errors='ignore')

metrics['isGeneratedByCopilot'] = False # metrics['agent'].apply(lambda x: True if x == 'Copilot' else False) # Corrected column name and capitalization

"""# Outcome variables

22. **Review time (reviewTime):** Time interval between the PR creation time and closed time in hours
"""

# Make sure we don't crash because the columns already exist (rentrant code)
metrics = metrics.drop(columns=['reviewTime'], errors='ignore')

# Calculate review time in hours, handling potential NaT values
metrics = metrics.assign(reviewTime=lambda x: (x['closed_at'] - x['created_at']).dt.total_seconds() / 3600)

# Fill N/A values with defaults (e.g., for open PRs)
metrics['reviewTime'] = metrics['reviewTime'].fillna(0)

"""23. **Is merged (state):** Whether or not a PR is merged (binary)

"""

# Make sure we don't crash because the columns already exist (reentrant code)
metrics = metrics.drop(columns=['isMerged'], errors='ignore')

# If Merged_At is None, the PR was not merged, otherwise it was
metrics['isMerged'] = metrics['merged_at'].apply(lambda x: 0 if x is None else 1)

"""# Order in CSV (control_metrics.csc)

1. **repoLanguage**
2. **forkCount**
3. **stargazerCount**
4. **repoAge**
5. **state**
6. **deletions**
7. **additions**
8. **changedFiles**
9. **commentsTotalCount**
10. **commitsTotalCount**
11. **prExperience**
12. **isMember**
13. **authorComments**
14. **reviewersComments**
15. **reviewersTotalCount**
16. **bodyLength**
17. **prSize**
18. **reviewTime**
19. **purpose**

Export to CSV
"""

metrics.to_csv(METRICS_CSV, index=False)

# Define the desired order of columns for the output CSVs
csv_order = ['repoLanguage',
'forkCount',
'stargazerCount',
'repoAge',
'state',
'deletions',
'additions',
'changedFiles',
'commentsTotalCount',
'commitsTotalCount',
'prExperience',
'isMember',
'authorComments',
'reviewersComments',
'reviewersTotalCount',
'bodyLength',
'prSize',
'reviewTime',
'purpose']

# Select and reorder columns for the treatment and control DataFrames
csv_metrics = metrics[csv_order].copy()

# Now, drop rows with NaN values from the column-filtered DataFrames
print(f"Number of control PRs before dropping NaNs: {len(csv_metrics):,}")
csv_metrics.dropna(inplace=True)
print(f"Number of control PRs after dropping NaNs: {len(csv_metrics):,}")

# Export to CSV
csv_metrics.to_csv(OUTPUT_CSV, index=False)

print(f"Exported control metrics to: {OUTPUT_CSV}")

"""# **Fourth**, Bot detection and filtering employed the methodology of Golzadehâ€¯etâ€¯al. (2022)

These are all human
"""